@misc{modded_nanogpt_2024,
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt}
}

@inproceedings{mueller2022transformers,
  title={Transformers Can Do {B}ayesian Inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022},
  url={https://openreview.net/forum?id=KSugKcbNf9}
}

@inproceedings{hollmann2023tabpfn,
  title={{TabPFN}: A Transformer That Solves Small Tabular Classification Problems in a Second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=cp5P_66LIn}
}

@article{hollmann2025tabpfnv2,
  author  = {Hollmann, Noah and Müller, Samuel and Purucker, Lennart and Krishnakumar, Arjun and Körfer, Max and Hoo, Shi Bin and Schirrmeister, Robin Tibor and Hutter, Frank},
  title   = {Accurate predictions on small data with a tabular foundation model},
  journal = {Nature},
  year    = {2025},
  volume  = {637},
  number  = {8045},
  pages   = {319--326},
  doi     = {10.1038/s41586-024-08328-6},
  url     = {https://doi.org/10.1038/s41586-024-08328-6},
  issn    = {1476-4687}
}

@misc{erickson2025tabarenalivingbenchmarkmachine,
  title={TabArena: A Living Benchmark for Machine Learning on Tabular Data},
  author={Nick Erickson and Lennart Purucker and Andrej Tschalzev and David Holzmüller and Prateek Mutalik Desai and David Salinas and Frank Hutter},
  year={2025},
  eprint={2506.16791},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.16791},
}

@misc{pfefferle2025nanotabpfnlightweighteducationalreimplementation,
  title={nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN},
  author={Alexander Pfefferle and Johannes Hog and Lennart Purucker and Frank Hutter},
  year={2025},
  eprint={2511.03634},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2511.03634},
}

@misc{jordan2024muon,
  author       = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and
                  Franz Cesista and Laker Newhouse and Jeremy Bernstein},
  title        = {Muon: An optimizer for hidden layers in neural networks},
  year         = {2024},
  url          = {https://kellerjordan.github.io/posts/muon/}
}

@misc{pytorch_sdpa_docs,
  author       = {{PyTorch Contributors}},
  title        = {torch.nn.functional.scaled\_dot\_product\_attention},
  year         = {2026},
  url          = {https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html},
  note         = {Accessed: 2026-02-11}
}

@misc{pytorch_amp_docs,
  author       = {{PyTorch Contributors}},
  title        = {Automatic Mixed Precision package - torch.amp},
  year         = {2026},
  url          = {https://pytorch.org/docs/stable/amp.html},
  note         = {Accessed: 2026-02-11}
}

@misc{pytorch_matmul_precision_docs,
  author       = {{PyTorch Contributors}},
  title        = {torch.set\_float32\_matmul\_precision},
  year         = {2026},
  url          = {https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html},
  note         = {Accessed: 2026-02-11}
}

@misc{pytorch_compile_docs,
  author       = {{PyTorch Contributors}},
  title        = {torch.compile},
  year         = {2026},
  url          = {https://pytorch.org/docs/stable/generated/torch.compile.html},
  note         = {Accessed: 2026-02-11}
}

@misc{pytorch_recompilation_docs,
  author       = {{PyTorch Contributors}},
  title        = {Dealing with Recompilations},
  year         = {2026},
  url          = {https://docs.pytorch.org/docs/stable/user_guide/torch_compiler/compile/programming_model.recompilation.html},
  note         = {Accessed: 2026-02-11}
}

@misc{geiping2022crammingtraininglanguagemodel,
      title={Cramming: Training a Language Model on a Single GPU in One Day},
      author={Jonas Geiping and Tom Goldstein},
      year={2022},
      eprint={2212.14034},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.14034},
}

@article{roratab2024,
  title={RoRA-Tab: Geometric Subspace Selection for Tabular Learning via Rotational Rank Adaptation},
  author={...},
  year={2024}
}

@article{das2025gompertz,
  title={Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics},
  author={Das, Indrashis and Safari, Mahmoud and Adriaensen, Steven and Hutter, Frank},
  journal={arXiv preprint arXiv:2502.03654},
  year={2025}
}

@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

