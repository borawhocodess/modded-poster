% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{caption}
% \usepackage[size=custom,width=118.9,height=84.1,scale=1.0]{beamerposter}
\usepackage[size=custom,width=84.1,height=118.9,scale=1.0]{beamerposter}


% \makeatletter
% \def\input@path{{colorthemes/}}
% \makeatother

\usetheme{gemini}
\usecolortheme{itu}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.033\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{
% \hspace*{\sepwidth}\RaggedRight
NanoTabPFN in \textasciitilde 10 minutes
% \newline\hspace*{\sepwidth}
}

\author{Salih \underline{Bora} Öztürk \inst{1} \and Alexander Pfefferle \inst{2,1} \and Frank Hutter \inst{3,2,1}}

\institute[shortinst]{\inst{1} University of Freiburg \samelineand \inst{2} ELLIS Institute Tübingen \samelineand \inst{3} Prior Labs}

% ====================
% Footer (optional)
% ====================

\footercontent{
  \parbox[c][8cm][c]{\textwidth}{
    \hspace{2em}
    \includegraphics[height=4.1cm]{img/logo_ellis_horizontal.png}
    \hfill
    \includegraphics[height=4.8cm]{img/logo_automl.png}
    \hfill
    \includegraphics[height=3.9cm]{img/logo_prior_horizontal.png}
    \hspace{3em}
  }
}
% (can be left out to remove footer)

% ====================
% Logo (optional)
% ====================

% use this to include logos on the left and/or right side of the header:
\logoright{\includegraphics[height=9cm]{img/logo_ufr_circle_black.png}}
% \logoleft{\includegraphics[height=7cm]{logo2.pdf}}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{0.46\paperwidth}

  \begin{alertblock}{TL;DR}

    \textbf{modded-nanoTabPFN}: This repository hosts the nanoTabPFN speedrun, in which we search for the fastest way to train a tabular foundation model that beats Random Forest on TabArena datasets.

  \end{alertblock}

  \begin{block}{Introduction}

    \heading{Motivation}

      \begin{itemize}
        \item TabPFN have shown that pretraining on synthetic datasets can lead to strong performance... \cite{hollmann2023tabpfn} \cite{hollmann2025tabpfnv2}
        \item However, training these models takes long, and no one has time to wait...
      \end{itemize}

    % TODO: good to highlight that back there it was just because of the small scale and no fancy tricks and here we scale up and do a bunch of fancy tricks
    % TODO: tricks cocktail?

    \heading{Background}

      % TODO: remove pfns
      % TODO: have this as a graphic?
      % TODO: GPT2 -> nanoGPT -> modded-nanoGPT with TabPFNv2 -> nanoTabPFN -> modded-nanoTabPFN
      % TODO: nanoGPT speedrun bulletpoints for people that don't know about it

      \begin{itemize}
        \item \textbf{PFN} prior fitted network... \cite{mueller2022transformers}
        \item \textbf{TabPFN} for tabular data... \cite{hollmann2023tabpfn} \cite{hollmann2025tabpfnv2}
        \item \textbf{nanoTabPFN} a lightweight and educational reimplementation of TabPFN \cite{pfefferle2025nanotabpfnlightweighteducationalreimplementation}
        \item \textbf{TabArena} a living benchmark for machine learning on tabular data \cite{erickson2025tabarenalivingbenchmarkmachine}
        \item \textbf{modded-nanogpt} OG speedrunning repo... \cite{modded_nanogpt_2024}
      \end{itemize}

    \heading{Goal}

      \begin{itemize}
        \item Pretrain a neural network to beat Random Forest (\textasciitilde 0.8068 validation average ROC AUC) on subsampled TabArena datasets using 1 NVIDIA L40S.
      \end{itemize}

    \heading{This repo now contains a training algorithm which attains the target performance in:}

      \begin{itemize}
        \item \textbf{9.26 minutes} on 1xL40S (baseline needed 74.32)
        \item \textbf{13184 synthetic datasets} (baseline needed 80576)
      \end{itemize}

  \end{block}

  % TODO: in rules/eval details we should clarify about what time gets meassured, and whether changes of prior are allowed/whether prior data generation gets meassured.

  \begin{block}{Rules}

    New records must:

    \begin{enumerate}
      \item Not modify the evaluation pipeline.
      \item Not load any pretrained weights.
      \item Run faster than prior record when baselined on the same hardware.
    \end{enumerate}

    Other than that, anything and everything is fair game!

  \end{block}

  \begin{block}{Evaluation details}

    Evaluation is on all 38 TabArena classification datasets:
    \begin{itemize}
      \item if >100 features, randomly select 100
      \item if >1000 rows, randomly select 1000 (stratified by class labels)
      \item 5-fold StratifiedKFold with shuffling, class labels are encoded with integers per fold
      \item average binary or one-vs-rest ROC AUC over all datasets
    \end{itemize}

    % TODO: add tabpfnv2 and v2.5 scores for fun?

  \end{block}

  \begin{block}{Improvement techniques}

    \heading{With the help of the following techniques:}
    \begin{itemize}
      \item Muon optimizer \cite{jordan2024muon}
      \item Batched Muon zeropower update for grouped QKV matrices
      \item Scaled Dot-Product Attention rewrite with explicit QKV \cite{pytorch_sdpa_docs}
      \item Pre-norm transformer blocks
      \item Compile TransformerEncoderLayer forward \cite{pytorch_compile_docs}
      \item bfloat16 autocast in training and inference \cite{pytorch_amp_docs}
      \item Set float32 matmul precision to high \cite{pytorch_matmul_precision_docs}
      \item Increase learning rate from $10^{-4}$ to $10^{-3}$
      \item Increase embedding size from 192 to 256
      \item Reduce attention heads from 6 to 4
    \end{itemize}

    \heading{Also these were tried but did not lead to improvements:}
    \begin{itemize}
      \item Xavier initialization
    \end{itemize}

  \end{block}

\end{column}

\separatorcolumn

\begin{column}{0.44\paperwidth}

  \begin{block}{Record history}

    % TODO: highlight these more

    \begin{figure}
      \centering
      \includegraphics[width=0.7\linewidth]{img/260211-032956-records-plot-x-time-y-val.png}
      \label{fig:record-history-time}
    \end{figure}

    \begin{figure}
      \centering
      \includegraphics[width=0.7\linewidth]{img/260211-032845-records-plot-x-datasets-y-val.png}
      \label{fig:record-history-datasets}
    \end{figure}

    \begin{center}
      {\small
      \setlength{\tabcolsep}{10pt}
      \begin{tabular}{@{}l p{0.36\linewidth} p{0.21\linewidth}@{}}
        \toprule
        Record time & Description & Contributors \\
        \midrule
        74.32 mins & Baseline & @borawhocodess, nanotabpfn contributors \\
        54.41 mins & Muon optimizer & @borawhocodess \\
        10.10 mins & SDPA, bf16, higher LR, wider embeddings, fewer heads & @carterprince \\
        9.26 mins & Batched Muon, compiled forward & @carterprince \\
        \bottomrule
      \end{tabular}
      }
    \end{center}

  \end{block}

  \vspace{1em}

  \begin{exampleblock}{Join the race!}

    \begin{figure}
      \centering
      \includegraphics[width=0.4\linewidth]{img/qr.png}
      \caption*{Scan to join the speedrun on GitHub\\{\footnotesize github.com/borawhocodess/modded-nanotabpfn}}
    \end{figure}

  \end{exampleblock}

  \begin{block}{References}

    \nocite{*}
    \footnotesize{\bibliographystyle{unsrt}\bibliography{poster}}

  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
